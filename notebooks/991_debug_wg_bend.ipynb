{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08c16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceviche_challenges\n",
    "from ceviche_challenges import units as u\n",
    "from ceviche_challenges.model_base import _wavelengths_nm_to_omegas\n",
    "\n",
    "from ceviche import viz, fdfd_ez\n",
    "from ceviche import jacobian\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as npa\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be791c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_design.brushes import notched_square_brush, circular_brush\n",
    "from inverse_design.conditional_generator import (\n",
    "    new_latent_design, transform\n",
    ")\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from javiche import jaxit\n",
    "\n",
    "from inverse_design.local_generator import generate_feasible_design_mask\n",
    "from jax.example_libraries.optimizers import adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba19d4",
   "metadata": {},
   "source": [
    "# Define the problem using ceviche_challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdd6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ceviche_challenges.waveguide_bend.prefabs.waveguide_bend_2umx2um_spec(\n",
    "    wg_width=400*u.nm, variable_region_size=(1600*u.nm, 1600*u.nm), cladding_permittivity=2.25\n",
    ")\n",
    "params = ceviche_challenges.waveguide_bend.prefabs.waveguide_bend_sim_params(resolution = 25 * u.nm,\n",
    "                                                                             wavelengths=u.Array([1270], u.nm))\n",
    "model = ceviche_challenges.waveguide_bend.model.WaveguideBendModel(params, spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4d640",
   "metadata": {},
   "source": [
    "# Define the optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a76d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_to_au(x):\n",
    "    return npa.power(10, x / 10)\n",
    "\n",
    "S_cutoff_dB = npa.array([-20., -0.5])\n",
    "S_cutoff = db_to_au(S_cutoff_dB)\n",
    "g = npa.array([-1 if x < S_cutoff.max() else +1 for x in S_cutoff])\n",
    "w_valid = npa.array([x if x < S_cutoff.max() else 1-x for x in S_cutoff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc72830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.89125094]\n",
      "[-1  1]\n",
      "[0.01       0.10874906]\n"
     ]
    }
   ],
   "source": [
    "print(S_cutoff)\n",
    "print(g)\n",
    "print(w_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9d2a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06601047 0.42532036] [9.9999997e-05 7.9432815e-01]\n",
      "[ 0.06591047 -0.3690078 ]\n",
      "[-0.06591047 -0.3690078 ]\n",
      "[ -6.591047 -36.90078 ]\n",
      "[1.3716612e-03 9.4231264e-17]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "S = np.random.rand(2)\n",
    "S = S / (1.1 * np.sum(S))\n",
    "\n",
    "print(jnp.abs(S) ** 2, jnp.abs(S_cutoff) ** 2)\n",
    "print(jnp.abs(S) ** 2 - jnp.abs(S_cutoff) ** 2)\n",
    "print(g * (jnp.abs(S) ** 2 - jnp.abs(S_cutoff) ** 2))\n",
    "print(g * (jnp.abs(S) ** 2 - jnp.abs(S_cutoff) ** 2) / jnp.min(w_valid))\n",
    "print(jax.nn.softplus(g * (jnp.abs(S) ** 2 - jnp.abs(S_cutoff) ** 2) / jnp.min(w_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea039c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_w = np.min(w_valid)\n",
    "\n",
    "# @jaxit()\n",
    "def objective_S(rho):\n",
    "    s_params, _ = model.simulate(rho)\n",
    "\n",
    "    s11 = npa.abs(s_params[:, 0, 0])\n",
    "    s21 = npa.abs(s_params[:, 0, 1])\n",
    "\n",
    "    return s11, s21\n",
    "\n",
    "def loss_function(S):\n",
    "    temp = g * (jnp.abs(S) ** 2 - jnp.abs(S_cutoff) ** 2) / jnp.min(w_valid)\n",
    "    return jnp.linalg.norm(jax.nn.softplus(temp)) ** 2\n",
    "\n",
    "def ad_softplus(x):\n",
    "    return npa.log(1 + npa.exp(x))\n",
    "\n",
    "def ad_loss_function(S):\n",
    "    temp = g * (npa.square(npa.abs(S)) - npa.square(npa.abs(S_cutoff))) / min_w\n",
    "    return npa.linalg.norm(ad_softplus(temp)) ** 2\n",
    "\n",
    "@jaxit()\n",
    "def ad_jaxed(rho):\n",
    "    S = objective_S(rho)\n",
    "    return ad_loss_function(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7793314d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# objective_S(latent)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m autograd\u001b[38;5;241m.\u001b[39melementwise_grad(objective_S)(\u001b[43mlatent\u001b[49m)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# autograd.value_and_grad(objective_S, latent)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent' is not defined"
     ]
    }
   ],
   "source": [
    "# objective_S(latent)\n",
    "autograd.elementwise_grad(objective_S)(latent).max()\n",
    "# autograd.value_and_grad(objective_S, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd781a",
   "metadata": {},
   "source": [
    "# Then, the inverse design aka the conditional generator part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(latent_weights, brush):\n",
    "    latent_t = transform(latent_weights, brush) #.reshape((Nx, Ny))\n",
    "    design_mask = generate_feasible_design_mask(latent_t, \n",
    "      brush, verbose=False)\n",
    "    design = (design_mask+1.0)/2.0\n",
    "    return design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "brush = circular_brush(5)\n",
    "latent = new_latent_design(model.design_variable_shape, bias=0.1, r=1, r_scale=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(latent):\n",
    "    design = forward(latent, brush)\n",
    "    # S = objective_S(design)\n",
    "    # return loss_function(S)\n",
    "    loss = ad_jaxed(design)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f54415",
   "metadata": {},
   "source": [
    "# Now we define the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs in the optimization\n",
    "Nsteps = 10\n",
    "# Parameters for the Adam optimizer\n",
    "step_size = 0.01\n",
    "beta1 = 0.667\n",
    "beta2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "init_fn, update_fn, params_fn = adam(step_size)\n",
    "state = init_fn(latent) #.flatten()\n",
    "#value_and_grad seems to have a problem. Figure out why!\n",
    "\n",
    "def step_fn(step, state):\n",
    "    latent = params_fn(state) # we need autograd arrays here...\n",
    "    grads = grad_fn(latent)\n",
    "    loss = loss_fn(latent)\n",
    "    #loss = loss_fn(latent)\n",
    "\n",
    "    optim_state = update_fn(step, grads, state)\n",
    "    # optim_latent = params_fn(optim_state)\n",
    "    # optim_latent = optim_latent/optim_latent.std()\n",
    "\n",
    "    # visualize_debug()\n",
    "    # visualize_all(latent)\n",
    "    return loss, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, update_fn, params_fn = adam(step_size)\n",
    "state = init_fn(npa.array(latent))\n",
    "\n",
    "latent = params_fn(state)\n",
    "grads = grad_fn(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d076e1a",
   "metadata": {},
   "source": [
    "#|eval:false\n",
    "range_ = trange(Nsteps)\n",
    "losses = np.ndarray(Nsteps)\n",
    "for step in range_:\n",
    "    print(step)\n",
    "    loss, state = step_fn(step, state)\n",
    "    losses[step] = loss\n",
    "    range_.set_postfix(loss=float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d46325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5d82d334f0157d2be4b279e89bf68849715c6d8952d96f6c35a4a723c2a5f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
